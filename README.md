# My A2C implementation as PL system

## The Algorithm


The separate parts:
- Data module
- Neural Nets
- PL module
- Callbacks
- Data set

## `CartPole-v0` parameters:
```

```
A2C net:
```

```

## `LunarLander-v2` parameters:
```

```
A2C net:
```

```

## Thanks to:

- [Environments in OpenAI (Leaderboard)](https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2)
- [PPO (OpenAI's blog)](https://openai.com/blog/openai-baselines-ppo/)
- [PPO implementation from Deep-Reinforcement-Learning-Hands-On-Second-Edition (page 606)](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter12/02_pong_a2c.py)
- [Optimization In Pytorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization)
- [Adam Grad - page 36 (Training NNs from Stanford's course)](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf)
- [Kullback–Leibler divergence (wikipedia)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
- [Kullback–Leibler divergence (YouTube video) - great](https://www.youtube.com/watch?v=ErfnhcEV1O8&ab_channel=Aur%C3%A9lienG%C3%A9ron)
